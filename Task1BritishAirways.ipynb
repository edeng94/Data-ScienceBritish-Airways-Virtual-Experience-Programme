{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHjo1Byldz8c"
      },
      "source": [
        "# Task 1: Web scraping to gain company inshights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkn3Kl3Udvqn"
      },
      "outputs": [],
      "source": [
        "#Import all libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "import gensim\n",
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "from gensim import corpora, models\n",
        "import pyLDAvis.gensim\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY9L4Mi5fdVt"
      },
      "source": [
        "### Section 1 - Web scraping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdnXFKJEd-9N"
      },
      "outputs": [],
      "source": [
        "# URL of the website to scrape\n",
        "url = 'https://www.airlinequality.com/airline-reviews/british-airways/page/1/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gbi60Y9texvL"
      },
      "outputs": [],
      "source": [
        "# List to store the extracted reviews\n",
        "reviewlist = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VArgrMboez-u"
      },
      "outputs": [],
      "source": [
        "# Function to get the BeautifulSoup object from a URL\n",
        "def get_soup(url):\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    return soup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3u9N7RKDe2ZF"
      },
      "outputs": [],
      "source": [
        "# Function to extract reviews from a BeautifulSoup object\n",
        "def get_reviews(soup):\n",
        "    reviews = soup.find_all('article', {'itemprop': 'review'})\n",
        "    try:\n",
        "        for item in reviews:\n",
        "            review = {\n",
        "                'title': item.find('h2', {'class': 'text_header'}).text,\n",
        "                'rating': item.find('div', {'itemprop': 'reviewRating'}).text.strip(),\n",
        "                'body': item.find('div', {'class': 'text_content'}).text.strip(),\n",
        "            }\n",
        "            reviewlist.append(review)\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzdzyxije48c"
      },
      "outputs": [],
      "source": [
        "# Loop through multiple pages of reviews\n",
        "for x in range(1, 362):\n",
        "    soup = get_soup(f'https://www.airlinequality.com/airline-reviews/british-airways/page/{x}/')\n",
        "    print(f'Getting page: {x}')\n",
        "    get_reviews(soup)\n",
        "    print(len(reviewlist))\n",
        "    if not soup.find('li', {'class': 'off', 'text': '$0'}):\n",
        "        pass\n",
        "    else:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voArVc5Ge6rn"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame from the extracted reviews\n",
        "df = pd.DataFrame(reviewlist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71CAFf56e8df"
      },
      "outputs": [],
      "source": [
        "# Save the DataFrame as a CSV file\n",
        "df.to_csv('BA-reviews.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FweszawUfj87"
      },
      "source": [
        "### Section 2 - Data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOqpi9R6fngx"
      },
      "outputs": [],
      "source": [
        "df['rating'] = df['rating'].replace('/10', '', regex=True).astype(float)\n",
        "df['verified'] = df['body'].str.contains('Trip Verified')\n",
        "df['body'] = df['body'].replace('âœ… Trip Verified ', '', regex=True)\n",
        "\n",
        "#Convert into lowercase\n",
        "df['body'] = df['body'].str.lower()\n",
        "df['title'] = df['title'].str.lower()\n",
        "\n",
        "#Remove punctuations\n",
        "df['body'] = df['body'].str.replace('[^\\w\\s]', '', regex=True)\n",
        "df['title'] = df['title'].str.replace('[^\\w\\s]', '', regex=True)\n",
        "\n",
        "#Remove numbers\n",
        "df['body'] = df['body'].str.replace('\\d', '', regex=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFLaMqoHfvi-"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "df['tokens'] = df['body'].apply(word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2t1xEGQf8R-"
      },
      "outputs": [],
      "source": [
        "# Load the spaCy English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to remove stopwords using spaCy\n",
        "def remove_stopwords(tokens):\n",
        "    text = ' '.join(tokens)\n",
        "    doc = nlp(text)\n",
        "    tokens_without_stopwords = [token.text for token in doc if not token.is_stop]\n",
        "    return tokens_without_stopwords\n",
        "\n",
        "df['tokens'] = df['tokens'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDTRhm3Kf8XP"
      },
      "outputs": [],
      "source": [
        "#Lemmatize\n",
        "\n",
        "# Function to lemmatize tokens\n",
        "def lemmatize(tokens):\n",
        "    text = ' '.join(tokens)\n",
        "    doc = nlp(text)\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    return lemmas\n",
        "\n",
        "df['tokens'] = df['tokens'].apply(lemmatize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDDFoPxvgMRU"
      },
      "source": [
        "### Section 3 - Topic modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPG6iKDzgSRP"
      },
      "outputs": [],
      "source": [
        "dictionary = corpora.Dictionary(df['tokens'])\n",
        "\n",
        "#Create document term matrix\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in df['tokens'] ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7DQAYFTgcM-"
      },
      "outputs": [],
      "source": [
        "lda = gensim.models.ldamodel.LdaModel\n",
        "num_topics=8\n",
        "%time ldamodel = lda(doc_term_matrix,num_topics=num_topics,id2word=dictionary,passes=50,minimum_probability=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c22IHr79gfGv"
      },
      "outputs": [],
      "source": [
        "ldamodel.print_topics(num_topics=num_topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spq_3-MRgfLQ"
      },
      "outputs": [],
      "source": [
        "lda_display = pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary, sort_topics=False, mds='mmds')\n",
        "\n",
        "pyLDAvis.display(lda_display)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muIXguYahvpv"
      },
      "source": [
        "### Section 4 - Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7M6pv39gnB_"
      },
      "outputs": [],
      "source": [
        "# Download the vader_lexicon resource\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Create the SentimentIntensityAnalyzer object\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to get sentiment scores for each list of tokens and return as separate columns\n",
        "def get_sentiment_scores(tokens):\n",
        "    text = ' '.join(tokens)\n",
        "    scores = sia.polarity_scores(text)\n",
        "    return scores['neg'], scores['neu'], scores['pos'], scores['compound']\n",
        "\n",
        "df[['negative', 'neutral', 'positive', 'compound']] = df['tokens'].apply(get_sentiment_scores).apply(pd.Series)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Htw2rgFVgsau"
      },
      "outputs": [],
      "source": [
        "def vader_analysis(compound):\n",
        "    if compound >= 0.5:\n",
        "        return 'Positive'\n",
        "    elif compound < 0:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "df['sentiment'] = df['compound'].apply(vader_analysis)\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8USIlAxOgwvA"
      },
      "outputs": [],
      "source": [
        "vader_counts = df['sentiment'].value_counts()\n",
        "vader_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VffsjLgagxWB"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.pie(vader_counts.values, labels = vader_counts.index, autopct='%1.1f%%', shadow=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMtSPqqBh2AV"
      },
      "source": [
        "### Section 5 - Wordclouds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6m9EY4Zgy6V"
      },
      "outputs": [],
      "source": [
        "# Combine all tokenized words into a single string\n",
        "all_tokens = ' '.join(df['tokens'].sum())\n",
        "\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "wc = WordCloud(\n",
        "    background_color='white',\n",
        "    stopwords=stopwords,\n",
        "    max_words=70,\n",
        "    max_font_size=30,\n",
        "    scale=3,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "wc.generate(all_tokens)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')  # Turn off axis\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}